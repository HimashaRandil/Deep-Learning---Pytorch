{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n# Install the dependencies\n\n# Dependencies to play around with the LLM\n%pip install -U bitsandbytes    # 8-bit optimizers and quantization functions (to compress the model size)\n%pip install -U transformers    # High-level wrapper to easily setup transformer-based neural networks\n%pip install -U accelerate      # High-level wrapper to easily integrate with multi GPU/TPU training & inference pipelines\n\n# Dependencies to conduct fine tuning\n%pip install -U peft            # Parameter-efficient fine-tuning (QLora, Lora etc.)\n%pip install -U trl             # Transformer Reinforcement Learning - Full-stack library to fine-tune and align LLMs\n%pip install -U datasets        # Datasets from Huggingface","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-04T07:34:05.755937Z","iopub.execute_input":"2024-05-04T07:34:05.756425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Authenticate into third-party platforms (WanDB to log our training progress and HuggingFace to save our pretrained model weights)\n\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nimport wandb\n\nuser_secrets = UserSecretsClient()\n\nsecret_hf = user_secrets.get_secret('HuggingFace')\nsecret_wandb = user_secrets.get_secret('WanDB')\n\nlogin(secret_hf)\nwandb.login(key=secret_wandb)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\ndef instantiate(model_path):\n    \n    # Model quantization configs\n    # Quantization allows us to work with a compressed version of the model (i.e. being able to fit into the GPU memory)\n    bnbConfig = BitsAndBytesConfig(\n        load_in_4bit = True,\n        bnb_4bit_quant_type = \"nf4\",\n        bnb_4bit_compute_dtype = torch.bfloat16,\n    )\n    \n    # Instantiate the model\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        device_map = 'auto',\n        quantization_config = bnbConfig\n    )\n\n    # Instantiate the tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    \n    return model, tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model_path = 'google/gemma-2b' #  '/kaggle/input/gemma/transformers/2b/2'\ninstruction_tuned_model_path = 'google/gemma-2b-it' # '/kaggle/input/gemma/transformers/2b-it/3'\n\nbase_model, base_tokenizer = instantiate(base_model_path)\nit_model, it_tokenizer = instantiate(instruction_tuned_model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"system = \"In the bustling streets of Victorian London, there exists a figure of unparalleled intellect and deductive prowess - Sherlock Holmes. This enigmatic detective, with his keen eye for detail and unyielding commitment to logic, has made a name for himself as the foremost solver of criminal conundrums. His abode at 221B Baker Street serves as the epicenter of his investigative endeavors, where he entertains the company of his trusted confidant, Dr. John Watson. Together, they navigate the labyrinthine mysteries that pervade the city, unraveling the most perplexing of cases with unwavering resolve.\"\nuser = \"How do you approach a new case, Sherlock?\"\n\nprompt = f\"<|system|>{system}</s> <|user|>{user}</s> <|assistant|>\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(prompt, tokenizer, model):\n    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n    return inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_input = tokenize(prompt, base_tokenizer, base_model)\nit_input = tokenize(prompt, it_tokenizer, it_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate(tokenized_input, tokenizer, model):\n    output = model.generate(**tokenized_input, max_length=1024, num_return_sequences=1)\n    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n    return decoded_output\n\ndef extract_response(generation):\n    return generation.split(\"<|assistant|>\")[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model_generation = generate(base_input, base_tokenizer, base_model)\nit_model_generation = generate(it_input, it_tokenizer, it_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extract_response(base_model_generation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extract_response(it_model_generation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finetuning!","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset_name = 'hieunguyenminh/roleplay'\ndataset = load_dataset(dataset_name, split=\"train[:300]\")\n\n# Have a look at the dataset\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove any records that have to do with Sherlock or Watson\ndataset = dataset.filter(lambda x: x['name'] != \"Sherlock\" and x['name'] != \"Watson\")\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    TrainingArguments,\n    logging,\n)\n\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\n\nfrom trl import SFTTrainer\n\n\ndef prepare_for_finetuning(model, tokenizer, model_output_dir):\n    \n    model.config.use_cache = False\n    model.config.pretraining_tp = 1\n    \n    tokenizer.padding_side = 'right'\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.add_eos_token = True\n    tokenizer.add_bos_token = True\n    \n    peft_config = LoraConfig(\n        lora_alpha = 16,\n        lora_dropout = 0.1,\n        r = 64,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules=['o_proj', 'q_proj', 'up_proj', 'v_proj', 'k_proj', 'down_proj', 'gate_proj']\n    )\n    \n    model = prepare_model_for_kbit_training(model)\n    model = get_peft_model(model, peft_config)\n    \n    training_arguments = TrainingArguments(\n        output_dir=model_output_dir,\n        num_train_epochs=1,\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=1,\n        optim=\"paged_adamw_32bit\",\n        save_strategy=\"epoch\",\n        logging_steps=100,\n        logging_strategy=\"steps\",\n        learning_rate=2e-4,\n        fp16=False,\n        bf16=False,\n        group_by_length=True,\n        report_to=None,\n    )\n    \n    trainer = SFTTrainer(\n        model = model,\n        train_dataset = dataset,\n        peft_config = peft_config,\n        max_seq_length = 512,\n        dataset_text_field = \"text\",\n        tokenizer = tokenizer,\n        args = training_arguments,\n        packing = False,\n    )\n    \n    return trainer, model\n    \n\ndef prepare_for_eval(model, tokenizer):\n    model.config.use_cache = True\n    model.gradient_checkpointing_disable()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_trainer, peft_base_model = prepare_for_finetuning(base_model, base_tokenizer, './gemma-2b-v2-role-play')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_base_model.print_trainable_parameters()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_trainer.model.save_pretrained('./gemma-2b-v2-role-play')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prepare_for_eval(peft_base_model, base_tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs = generate(base_input, base_tokenizer, peft_base_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extract_response(outputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}